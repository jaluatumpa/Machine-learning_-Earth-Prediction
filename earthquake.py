# -*- coding: utf-8 -*-
"""Earthquake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nsT-h_BnyKl-BeLaAAHZM1MGc_KLqsiB
"""

## importing library

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/database.csv.zip')
df.head()

# title
import seaborn as sns
corrM=df.corr()
dataplot = sns.heatmap(corrM, cmap="YlGnBu", annot=True)

## We can eliminate the features of Type,Depth Error,Depth Seismic Stations	,Magnitude Type,	...	,Magnitude Seismic Stations,Azimuthal Gap,orizontal Distance,Horizontal Error,Root Mean Square,ID,Source,Location Source,Magnitude Source,	Status	which are highly multicorrelated with Date,Time,	Latitude,	Longitude,Depth	,Magnitude.

df=df[['Date','Time','Latitude','Longitude','Depth','Magnitude']]
df.head()

# Check null value
df.isnull().sum()

df.columns

## Since the data is random,so we need to scale it based on the model inputs.In this case convert the given data and time into unix time which is a seconds and number.

import datetime
import time
timestamp=[]
for d, t in zip(df['Date'], df['Time']):
    try:
        ts = datetime.datetime.strptime(d+' '+t, '%m/%d/%Y %H:%M:%S')
        timestamp.append(time.mktime(ts.timetuple()))
    except ValueError:
        timestamp.append('ValueError')
timeStamp = pd.Series(timestamp)
df['Timestamp'] = timeStamp.values
final_df = df.drop(['Date', 'Time'], axis=1)
final_df = final_df[final_df.Timestamp != 'ValueError']
final_df.head()

pip install basemap

## Visualization

from mpl_toolkits.basemap import Basemap

m = Basemap(projection='mill',llcrnrlat=-80,urcrnrlat=80, llcrnrlon=-180,urcrnrlon=180,lat_ts=20,resolution='c')

longitudes = df["Longitude"].tolist()
latitudes = df["Latitude"].tolist()

x,y = m(longitudes,latitudes)

fig = plt.figure(figsize=(12,10))
plt.title("All affected areas")
m.plot(x, y, "o", markersize = 2, color = 'blue')
m.drawcoastlines()
m.fillcontinents(color='coral',lake_color='aqua')
m.drawmapboundary()
m.drawcountries()
plt.show()

from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
m=Basemap(projection='mill',llcrnrlat=-80,urcrnrlat=80,llcrnrlon=-180,urcrnrlon=180,lat_ts=20,resolution='c')
longitudes=df["Longitude"].tolist()
latitudes=df["Latitude"].tolist()
x,y=m(longitudes,latitudes)
fig=plt.figure(figsize=(12,10))
plt.title("All affected areas(Dark Mode)")
#set color for map element
m.drawcoastlines(color='#555555')
m.drawmapboundary(fill_color='#333333')
m.fillcontinents(color='#666666',lake_color='#333333')
m.drawcountries(color='#777777')
#plot data point
m.plot(x,y,"o",markersize=2,color='blue')
plt.show()

## Splitting the data into training and testing
X=final_df[['Timestamp','Latitude','Longitude']]
y=final_df[['Magnitude','Depth']]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, X_test.shape)

## Neuron Network

from keras.models import Sequential
from keras.layers import Dense

def create_model(neurons, activation, optimizer, loss):
    model = Sequential()
    model.add(Dense(neurons, activation=activation, input_shape=(3,)))
    model.add(Dense(neurons, activation=activation))
    model.add(Dense(2, activation='softmax'))
    
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
    
    return model

from keras.wrappers.scikit_learn import KerasClassifier

model = KerasClassifier(build_fn=create_model, verbose=0)

neurons = [16]
batch_size = [10]
epochs = [10]
activation = ['sigmoid', 'relu']
optimizer = ['SGD', 'Adadelta']
loss = ['squared_hinge']

param_grid = dict(neurons=neurons, batch_size=batch_size, epochs=epochs, activation=activation, optimizer=optimizer, loss=loss)

X_train=np.asarray(X_train).astype(np.float32)
y_train=np.asarray(y_train).astype(np.float32)

from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X_train, y_train)

print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

X_test=np.asarray(X_test).astype(np.float32)
y_test=np.asarray(y_test).astype(np.float32)

model = Sequential()
model.add(Dense(16, activation='relu', input_shape=(3,)))
model.add(Dense(16, activation='relu'))
model.add(Dense(2, activation='softmax'))

model.compile(optimizer='SGD', loss='squared_hinge', metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data=(X_test, y_test))

[test_loss, test_acc] = model.evaluate(X_test, y_test)
print("Evaluation result on Test Data : Loss = {}, accuracy = {}".format(test_loss, test_acc))

# So we can see the above output that our neural network model for earthquake prediction performs well